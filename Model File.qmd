---
title: "Modeling File"
author: "Alise Miller"
format: html
editor: visual
---

## Introduction

This project uses the 2015 BRFSS Diabetes Health Indicators data set. We will explore and model Diabetes_binary (yes or no to having diabetes) with logistic regression model, classification trees, and random forest. The predictors include: Education, Income, Fruits, Physical Health and Health Care Coverage. Some of these predictors ahve a stronger impact than others. We will then decide the best model and have some discussion. 

```{r}
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))

source("transferable_EDA_for_Model.R")

set.seed(22)
diabetes_data_split <- initial_split(EDA_diabetes_data, prop = 0.70) 
diabetes_data_training <-training (diabetes_data_split) 
diabetes_data_test <-testing (diabetes_data_split) 
diabetes_CV_folds <- vfold_cv(diabetes_data_training, 5)


```

## Logistic Regression Models

### Explanation

A logistic model is a model that can use both categorical and numeric variables, and we use it to gauge the likelihood of a binary response happening. This could be something that would be yes or no, true or false kind of variable. In this case we use it to predict the likelihood of someone having diabetes, based on the variables below. A linear model would not work here because of the type of variables being used. Now we will create a few logistic models for the response Diabetes_binary, whether someone has diabetes or not. The first model will be based on education level, and physical health. Model 2 will be based on education level, physical health, Any healthcare coverage and income. Model 3 will be based on education level, physical health, Any healthcare coverage,income, fruit consumption.

```{r}
LR1_recipe <- recipe(Diabetes_binary_fac ~ Education_fac + PhysHlth, data = diabetes_data_training ) |>
  step_normalize(PhysHlth)
LR2_recipe <- recipe(Diabetes_binary_fac ~ Education_fac + PhysHlth + Health_Care_Coverage + Income, data = diabetes_data_training ) |>
  step_normalize(PhysHlth)
LR3_recipe <-recipe(Diabetes_binary_fac ~ Education_fac + PhysHlth + Health_Care_Coverage + Income + Fruits, data = diabetes_data_training ) |>
  step_normalize(PhysHlth)

LR_spec <- logistic_reg() |>
  set_engine("glm")
LR1_wkf <-workflow() |>
  add_recipe(LR1_recipe) |>
  add_model(LR_spec)
LR2_wkf <-workflow() |>
  add_recipe(LR2_recipe) |>
  add_model(LR_spec)
LR3_wkf <-workflow() |>
  add_recipe(LR3_recipe) |>
  add_model(LR_spec)
 
 LR1_fit <- LR1_wkf |>
  fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))
 LR2_fit <- LR2_wkf |>
  fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))
 LR3_fit <- LR3_wkf |>
  fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))
 
LR_metrics <-rbind(LR1_fit |> collect_metrics(),
      LR2_fit |> collect_metrics(),
      LR3_fit |> collect_metrics()) |>
  mutate(Model = c("Model1", "Model1", "Model2", "Model2", "Model3", "Model3")) |>
  select(Model, everything())

LR_metrics
```

### Results/Choice

According to the Logistic Regression that I just ran Model 3 would be the best because it has a lower log-loss. But Model 2 is very close behind if we only keep 3 or 4 decimals they would be approximately the same.

## Classification tree

A classification tree is a type of decision tree where the response variable is categorical, binary. In our case subject has diabetes yes or no. It works by using certain rules to repeatedly split the training data into smaller sections. Each time the data is split a new branch is made. This process is continued until certain conditions are met based on the rule the machine is using. To make it so the tree wouldn't grow too large and overfitting the training set we use a cost_complexity parameter.  One can use a classification tree if they want an easy to interpret visual representation of the decision making process. Classification trees can be used if the response variable is categorical, binary. For this I used Level of education, number of poor physical health days in the a month, and health care coverage as the predictors for having diabetes or not. 

```{r}

train_tree <- diabetes_data_training |>
  select(-Diabetes_binary, -AnyHealthcare, -Education)
test_tree <-diabetes_data_test |>
  select(-Diabetes_binary, -AnyHealthcare, -Education)

tree_rec <- recipe(Diabetes_binary_fac ~ Education_fac + PhysHlth + Health_Care_Coverage, data = train_tree)

tree_rec

tree_mod <- decision_tree( tree_depth = tune(),
 min_n = 20, 
 cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

tree_wkf <-workflow() |>
  add_recipe(tree_rec) |>
  add_model(tree_mod)

tree_grid <- grid_regular(cost_complexity(),
tree_depth(),
levels = c(10, 5))

my_metrics <- metric_set(accuracy, roc_auc, brier_class, mn_log_loss)

tree_fits <- tree_wkf |> 
  tune_grid(resamples = diabetes_CV_folds,
                grid = tree_grid,
            metrics = my_metrics)
tree_fits

tree_fits |>
  collect_metrics()

tree_fits %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)

tree_fits |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)
tree_best_params <- select_best(tree_fits, metric = "mn_log_loss")

tree_best_params

tree_final_wkf <- tree_wkf |>
  finalize_workflow(tree_best_params)

tree_final_fit <- tree_final_wkf |>
  last_fit(diabetes_data_split)

tree_final_fit

tree_final_fit |>
  collect_metrics()

tree_final_model <- extract_workflow(tree_final_fit) 
tree_final_model

tree_final_model %>%
  extract_fit_engine() %>%
  rpart.plot::rpart.plot(roundint = FALSE)
```

### Results

The tree begins by asking if a person has had less than 7.5 days of poor physical health. If the answer is yes, we go left, and if the answer is no, we go right. The left side predicts no diabetes, with 84% of all observations that took this path resulting in no diabetes. And the tree continues for people who had more than 7.5 days of poor health days to ask Education level: some college/college grad. If yes, we again go to the left, and if no, then to the right, and the tree continues. The left side means that of all the observations that had some college/college grad 9% were predicted not to have diabetes. The tree goes on to split into education level high school grad, Health care coverage, education level none/some high school, poor physical health days less than 17,  poor physical health days greater than or equal to 13, poor physical health days less than 8.5,  poor physical health days less than 11, and lastly poor physical health days greater than or equal to 25.  

## Random Forest

### Explanation
A Random Forest is a type of ensemble tree that creates multiple trees from bootstrap samples. Each tree is trained on a small sample size of predictors in an effort to reduce overfitting. So it makes a tree diagram, gets the results of that tree using the training data, and then it repeats the process as many times as required. Then it averages the results in some way for a final prediction. Random forest could be used if you are not interested in testing all of the predictors, but you do have a mix of categorical and quantitative variables. Random forest should also be considered if you are less concerned about the interpretation of individual predictors.

```{r}
suppressPackageStartupMessages(library(parsnip))
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(ranger))


rf_spec <- rand_forest(mtry = tune(), trees = 800, min_n = 20) |>
  set_engine("ranger") |>
  set_mode("classification")

rf_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(rf_spec)

rf_fit <-rf_wkf |>
  tune_grid(
    resamples = diabetes_CV_folds,
    grid = 4, 
    metrics = my_metrics)

rf_fit |>
  collect_metrics() |> 
  filter(.metric == "mn_log_loss") |>
  arrange(mean)

rf_best_params <- select_best(rf_fit, metric ="mn_log_loss")

rf_final_wk <- finalize_workflow(rf_wkf, rf_best_params)

rf_final_fit <- rf_final_wk |>
  last_fit(diabetes_data_split, metrics = metric_set(accuracy, mn_log_loss))

rf_final_fit |> collect_metrics()
```

### Results
After fitting the random forest model, I tuned with mtry and min_n. I collected the metrics of accuracy and log loss. The best model had an mtry of 2 with a log loss of 0.384. 

## Final Model Selection 

### Logistic Regression Model 
Below is the testing of the third Logistic Regression model. 
```{r}
 LR3_wkf |> 
  last_fit(diabetes_data_split,
                     metrics = metric_set(accuracy, mn_log_loss)) |> 
  collect_metrics()
```
The log loss on the test data set is 0.383, while the log loss on the trained data was 0.380. This shows that the model was consistent in predicting the test set. We should be good to use it on other unused data. 

### Classification Tree

I already ran the prediction on the test set above for the Classification Tree, but here it is again. 
```{r}
tree_final_wkf <- tree_wkf |>
  finalize_workflow(tree_best_params)

tree_final_fit <- tree_final_wkf |>
  last_fit(diabetes_data_split, metrics = my_metrics)

tree_final_fit
tree_final_fit |>
  collect_metrics()
tree_final_model <- extract_workflow(tree_final_fit) 
tree_final_model

tree_final_model %>%
  extract_fit_engine() %>%
  rpart.plot::rpart.plot(roundint = FALSE)
```
The log loss for the trained data was 0.389 and the log loss for the tested set was 0.394. It seems that the model was pretty consistent in predicting the on the test data. I will note that the 0.394 is slightly less confident. 

### Random Forest 
I already ran the prediction on the test set above for the Random Forest model, but here it is again. 

```{r}
rf_final_fit <- rf_final_wk |>
  last_fit(diabetes_data_split, metrics = metric_set(accuracy, mn_log_loss))

rf_final_fit |> collect_metrics()
```
This shows that when the best model from the trained set is now used on the tested set the log loss is 0.387 and the accuracy is 0.860 while the trained data set had a log loss of 0.384. These numbers align with the results of the training set. I would move to say this is a good model. 

### The WINNER

This is a tough to decide. The winner I choose is Logistic Regression model. Here are the things I considered while making this decision.
Random Forest model training and test set log loss are only 0.003 from each other, but interpreting the model could prove to be challenging. But Random Forest tend to be highly accurate models. And from this project experience they tend to take more time.  While interpreting the classification tree is easier, the log loss metrics for the test and training are the furthest away from each other at 0.005. And classification trees tend to be less accurate models. Logistic Regression model training and test sets log losses are also 0.003 from each other. But logistic regression do not work well with non linear data. Looking at another metric, accuracy on the test set,  both Classification Tree and Logistic regression model have an accuracy of 0.860. To no surprise Random Forest has a higher accuracy of 0.865.  

[Click here to return to EDA Page](EDA.html)